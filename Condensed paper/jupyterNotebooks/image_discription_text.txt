{'other_images/Figure_2_0.png': 'Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\n\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\n', 'other_images/Figure_4_0.png': 'Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\ntion embeddings and the position embeddings.\n', 'other_images/Figure_6_0.png': 'Table 2: SQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training check-\npoints and fine-tuning seeds.\n', 'other_images/Figure_6_8.png': 'Table 4: SWAG Dev and Test accuracies. ‘Human per-\nformance is measured with 100 samples, as reported in\nthe SWAG paper.\n', 'other_images/Figure_7_0.png': 'Table 5: Ablation over the pre-training tasks using the\nBERTgase architecture. “No NSP” is trained without\nthe next sentence prediction task. “LTR & No NSP” is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\ndomly initialized BiLSTM on top of the “LTR + No\nNSP” model during fine-tuning.\n', 'other_images/Table_8_3.png': 'Table 6: Ablation over BERT model size. #L = the\nnumber of layers; #H = hidden size; #A = number of at-\ntention heads. “LM (ppl)” is the masked LM perplexity\nof held-out training data.\n', 'other_images/Figure_8_5.png': 'Table 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\n', 'other_images/Figure_12_0.png': 'Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\n', 'other_images/Figure_14_0.png': 'Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.\n', 'other_images/Figure_14_6.png': '', 'other_images/Figure_15_4.png': 'Figure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after fine-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.\n', 'other_images/Table_15_7.png': ''}