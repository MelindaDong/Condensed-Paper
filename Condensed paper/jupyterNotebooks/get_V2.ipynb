{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add other images\n",
    "### use GPT and context dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\n",
      "\n",
      "### Overview:\n",
      "\n",
      "The paper tackles the problematic area of question answering tasks from passages in Wikipedia, addressing the specific prediction of an answer text span within a given passage and tackling unanswerable questions. BERT, a novel, conceptually simple yet potent model, is presented as the paper's primary contribution. BERT is optimized for deep bidirectional architectures, offering applicability across a wide range of Natural Language Processing tasks, exemplified by achieving state-of-the-art results across eleven such tasks. The paper utilizes various datasets, such as the GLUE benchmark, SQuAD v1.1, and the SWAG dataset. Ablation studies were conducted to explore aspects of BERT and the importance of its deep bidirectionality was a notable discovery. Despite various model alterations, BERT's effectiveness remained evident, particularly for Named Entity Recognition tasks. The paper concludes by affirming BERT's simplicity and efficacy in achieving exceptional results on multiple NLP tasks, emphasizing the crucial role of unsupervised pre-training in today's language understanding systems.\n",
      "\n",
      "### Detailed Method:\n",
      "\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a language model that has two key stages: pre-training and fine-tuning. In the pre-training phase, the model is trained on a range of tasks without labeled data. The fine-tuning phase involves using labeled data from specific tasks, which is where the pre-trained parameters of the BERT model are fine-tuned for targeted performance. \n",
      "\n",
      "A key feature of BERT is its model architecture, which is a multi-layer bidirectional Transformer encoder. The number of layers is represented as L, the hidden size as H, and the number of self-attention heads as A. Two model sizes are used: BERT BASE and BERT LARGE. The main distinction between these is that BERT uses bidirectional self-attention while other models use self-attention that only conditions to the context on the left of each token.\n",
      "\n",
      "BERT's input representation allows for representation of both single sentences or pairs of sentences. The first token in every sequence is a special classification token ([CLS]) that helps understand aggregated sequence representation. Sentence pairs have a special token ([SEP]) to separate them and to identify their separate embeddings (Segment A or B), as shown in Figure 2.\n",
      "\n",
      "During pre-training, BERT uses two unsupervised tasks. The first involves masking a certain portion of the input tokens and then predicting those masked tokens — this approach is called \"Masked LM\". BERT also trains for a \"Next Sentence Prediction (NSP)\" task, which helps the model understand the relationship between sentences. The final hidden vector of the special [CLS] token (C) is used for this prediction.\n",
      "\n",
      "To enhance performance on various tasks like question answering and natural language inference, the authors 'fine-tuned' BERT. The advantage of BERT's method is that a wide range of tasks can be handled by simply changing the inputs and outputs even in task-specific areas. Fine-tuning is much quicker and cheaper than pre-training; it can be completed in just an hour on a cloud TPU or a few hours on a GPU. \n",
      "\n",
      "Overall, BERT's approach combines unlabeled pre-training with task-specific fine-tuning, leading to a powerful and versatile model for NLP tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read V1_summary as text\n",
    "with open ('V1_summary.md', 'r') as f:\n",
    "    V1_summary = f.read()\n",
    "\n",
    "print(V1_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>discription</th>\n",
       "      <th>key_words</th>\n",
       "      <th>keys</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other_images/Figure_2_0.png</td>\n",
       "      <td>Figure 1: Overall pre-training and fine-tuning...</td>\n",
       "      <td>Figure 1:</td>\n",
       "      <td>['Fig. 1', 'Figure 1', 'Figure 1:']</td>\n",
       "      <td>[[], [' TokM  Masked Sentence A Masked Sentenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other_images/Figure_4_0.png</td>\n",
       "      <td>Figure 2: BERT input representation. The input...</td>\n",
       "      <td>Figure 2:</td>\n",
       "      <td>['Fig. 2', 'Figure 2', 'Figure 2:']</td>\n",
       "      <td>[[], [' A visualiza- tion of this construction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other_images/Figure_6_0.png</td>\n",
       "      <td>Table 2: SQuAD 1.1 results. The BERT ensemble\\...</td>\n",
       "      <td>Table 2:</td>\n",
       "      <td>['Table 2', 'Table 2:']</td>\n",
       "      <td>[[' Table 2 shows top leaderboard entries as w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other_images/Figure_6_8.png</td>\n",
       "      <td>Table 4: SWAG Dev and Test accuracies. ‘Human ...</td>\n",
       "      <td>Table 4:</td>\n",
       "      <td>['Table 4', 'Table 4:']</td>\n",
       "      <td>[['0 Table 4: SWAG Dev and Test accuracies', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other_images/Figure_7_0.png</td>\n",
       "      <td>Table 5: Ablation over the pre-training tasks ...</td>\n",
       "      <td>Table 5:</td>\n",
       "      <td>['Table 5', 'Table 5:']</td>\n",
       "      <td>[['9 Table 5: Ablation over the pre-training t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other_images/Table_8_3.png</td>\n",
       "      <td>Table 6: Ablation over BERT model size. #L = t...</td>\n",
       "      <td>Table 6:</td>\n",
       "      <td>['Table 6', 'Table 6:']</td>\n",
       "      <td>[[' Results on selected GLUE tasks are shown i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other_images/Figure_8_5.png</td>\n",
       "      <td>Table 7: CoNLL-2003 Named Entity Recognition r...</td>\n",
       "      <td>Table 7:</td>\n",
       "      <td>['Table 7', 'Table 7:']</td>\n",
       "      <td>[['5 - Table 7: CoNLL-2003 Named Entity Recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other_images/Figure_12_0.png</td>\n",
       "      <td>Figure 3: Differences in pre-training model ar...</td>\n",
       "      <td>Figure 3:</td>\n",
       "      <td>['Fig. 3', 'Figure 3', 'Figure 3:']</td>\n",
       "      <td>[[], ['Figure 3: Differences in pre-training m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other_images/Figure_14_0.png</td>\n",
       "      <td>Figure 4: Illustrations of Fine-tuning BERT on...</td>\n",
       "      <td>Figure 4:</td>\n",
       "      <td>['Fig. 4', 'Figure 4', 'Figure 4:']</td>\n",
       "      <td>[[], ['5 Illustrations of Fine-tuning on Diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>other_images/Figure_14_6.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>other_images/Figure_15_4.png</td>\n",
       "      <td>Figure 5: Ablation over number of training ste...</td>\n",
       "      <td>Figure 5:</td>\n",
       "      <td>['Fig. 5', 'Figure 5', 'Figure 5:']</td>\n",
       "      <td>[[], ['1 Effect of Number of Training Steps Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>other_images/Table_15_7.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0    other_images/Figure_2_0.png   \n",
       "1    other_images/Figure_4_0.png   \n",
       "2    other_images/Figure_6_0.png   \n",
       "3    other_images/Figure_6_8.png   \n",
       "4    other_images/Figure_7_0.png   \n",
       "5     other_images/Table_8_3.png   \n",
       "6    other_images/Figure_8_5.png   \n",
       "7   other_images/Figure_12_0.png   \n",
       "8   other_images/Figure_14_0.png   \n",
       "9   other_images/Figure_14_6.png   \n",
       "10  other_images/Figure_15_4.png   \n",
       "11   other_images/Table_15_7.png   \n",
       "\n",
       "                                          discription  key_words  \\\n",
       "0   Figure 1: Overall pre-training and fine-tuning...  Figure 1:   \n",
       "1   Figure 2: BERT input representation. The input...  Figure 2:   \n",
       "2   Table 2: SQuAD 1.1 results. The BERT ensemble\\...  Table 2:    \n",
       "3   Table 4: SWAG Dev and Test accuracies. ‘Human ...  Table 4:    \n",
       "4   Table 5: Ablation over the pre-training tasks ...  Table 5:    \n",
       "5   Table 6: Ablation over BERT model size. #L = t...  Table 6:    \n",
       "6   Table 7: CoNLL-2003 Named Entity Recognition r...  Table 7:    \n",
       "7   Figure 3: Differences in pre-training model ar...  Figure 3:   \n",
       "8   Figure 4: Illustrations of Fine-tuning BERT on...  Figure 4:   \n",
       "9                                                 NaN        NaN   \n",
       "10  Figure 5: Ablation over number of training ste...  Figure 5:   \n",
       "11                                                NaN        NaN   \n",
       "\n",
       "                                   keys  \\\n",
       "0   ['Fig. 1', 'Figure 1', 'Figure 1:']   \n",
       "1   ['Fig. 2', 'Figure 2', 'Figure 2:']   \n",
       "2               ['Table 2', 'Table 2:']   \n",
       "3               ['Table 4', 'Table 4:']   \n",
       "4               ['Table 5', 'Table 5:']   \n",
       "5               ['Table 6', 'Table 6:']   \n",
       "6               ['Table 7', 'Table 7:']   \n",
       "7   ['Fig. 3', 'Figure 3', 'Figure 3:']   \n",
       "8   ['Fig. 4', 'Figure 4', 'Figure 4:']   \n",
       "9                                   NaN   \n",
       "10  ['Fig. 5', 'Figure 5', 'Figure 5:']   \n",
       "11                                  NaN   \n",
       "\n",
       "                                             contexts  \n",
       "0   [[], [' TokM  Masked Sentence A Masked Sentenc...  \n",
       "1   [[], [' A visualiza- tion of this construction...  \n",
       "2   [[' Table 2 shows top leaderboard entries as w...  \n",
       "3   [['0 Table 4: SWAG Dev and Test accuracies', '...  \n",
       "4   [['9 Table 5: Ablation over the pre-training t...  \n",
       "5   [[' Results on selected GLUE tasks are shown i...  \n",
       "6   [['5 - Table 7: CoNLL-2003 Named Entity Recogn...  \n",
       "7   [[], ['Figure 3: Differences in pre-training m...  \n",
       "8   [[], ['5 Illustrations of Fine-tuning on Diffe...  \n",
       "9                                                  []  \n",
       "10  [[], ['1 Effect of Number of Training Steps Fi...  \n",
       "11                                                 []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# read image_discrption_df.csv as dataframe\n",
    "image_discrption_df = pd.read_csv('image_discription_df.csv')\n",
    "image_discrption_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>discription</th>\n",
       "      <th>key_words</th>\n",
       "      <th>keys</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other_images/Figure_2_0.png</td>\n",
       "      <td>Figure 1: Overall pre-training and fine-tuning...</td>\n",
       "      <td>Figure 1:</td>\n",
       "      <td>['Fig. 1', 'Figure 1', 'Figure 1:']</td>\n",
       "      <td>[[], [' TokM  Masked Sentence A Masked Sentenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other_images/Figure_4_0.png</td>\n",
       "      <td>Figure 2: BERT input representation. The input...</td>\n",
       "      <td>Figure 2:</td>\n",
       "      <td>['Fig. 2', 'Figure 2', 'Figure 2:']</td>\n",
       "      <td>[[], [' A visualiza- tion of this construction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other_images/Figure_6_0.png</td>\n",
       "      <td>Table 2: SQuAD 1.1 results. The BERT ensemble\\...</td>\n",
       "      <td>Table 2:</td>\n",
       "      <td>['Table 2', 'Table 2:']</td>\n",
       "      <td>[[' Table 2 shows top leaderboard entries as w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other_images/Figure_6_8.png</td>\n",
       "      <td>Table 4: SWAG Dev and Test accuracies. ‘Human ...</td>\n",
       "      <td>Table 4:</td>\n",
       "      <td>['Table 4', 'Table 4:']</td>\n",
       "      <td>[['0 Table 4: SWAG Dev and Test accuracies', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other_images/Figure_7_0.png</td>\n",
       "      <td>Table 5: Ablation over the pre-training tasks ...</td>\n",
       "      <td>Table 5:</td>\n",
       "      <td>['Table 5', 'Table 5:']</td>\n",
       "      <td>[['9 Table 5: Ablation over the pre-training t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other_images/Table_8_3.png</td>\n",
       "      <td>Table 6: Ablation over BERT model size. #L = t...</td>\n",
       "      <td>Table 6:</td>\n",
       "      <td>['Table 6', 'Table 6:']</td>\n",
       "      <td>[[' Results on selected GLUE tasks are shown i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other_images/Figure_8_5.png</td>\n",
       "      <td>Table 7: CoNLL-2003 Named Entity Recognition r...</td>\n",
       "      <td>Table 7:</td>\n",
       "      <td>['Table 7', 'Table 7:']</td>\n",
       "      <td>[['5 - Table 7: CoNLL-2003 Named Entity Recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other_images/Figure_12_0.png</td>\n",
       "      <td>Figure 3: Differences in pre-training model ar...</td>\n",
       "      <td>Figure 3:</td>\n",
       "      <td>['Fig. 3', 'Figure 3', 'Figure 3:']</td>\n",
       "      <td>[[], ['Figure 3: Differences in pre-training m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other_images/Figure_14_0.png</td>\n",
       "      <td>Figure 4: Illustrations of Fine-tuning BERT on...</td>\n",
       "      <td>Figure 4:</td>\n",
       "      <td>['Fig. 4', 'Figure 4', 'Figure 4:']</td>\n",
       "      <td>[[], ['5 Illustrations of Fine-tuning on Diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>other_images/Figure_15_4.png</td>\n",
       "      <td>Figure 5: Ablation over number of training ste...</td>\n",
       "      <td>Figure 5:</td>\n",
       "      <td>['Fig. 5', 'Figure 5', 'Figure 5:']</td>\n",
       "      <td>[[], ['1 Effect of Number of Training Steps Fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0    other_images/Figure_2_0.png   \n",
       "1    other_images/Figure_4_0.png   \n",
       "2    other_images/Figure_6_0.png   \n",
       "3    other_images/Figure_6_8.png   \n",
       "4    other_images/Figure_7_0.png   \n",
       "5     other_images/Table_8_3.png   \n",
       "6    other_images/Figure_8_5.png   \n",
       "7   other_images/Figure_12_0.png   \n",
       "8   other_images/Figure_14_0.png   \n",
       "10  other_images/Figure_15_4.png   \n",
       "\n",
       "                                          discription  key_words  \\\n",
       "0   Figure 1: Overall pre-training and fine-tuning...  Figure 1:   \n",
       "1   Figure 2: BERT input representation. The input...  Figure 2:   \n",
       "2   Table 2: SQuAD 1.1 results. The BERT ensemble\\...  Table 2:    \n",
       "3   Table 4: SWAG Dev and Test accuracies. ‘Human ...  Table 4:    \n",
       "4   Table 5: Ablation over the pre-training tasks ...  Table 5:    \n",
       "5   Table 6: Ablation over BERT model size. #L = t...  Table 6:    \n",
       "6   Table 7: CoNLL-2003 Named Entity Recognition r...  Table 7:    \n",
       "7   Figure 3: Differences in pre-training model ar...  Figure 3:   \n",
       "8   Figure 4: Illustrations of Fine-tuning BERT on...  Figure 4:   \n",
       "10  Figure 5: Ablation over number of training ste...  Figure 5:   \n",
       "\n",
       "                                   keys  \\\n",
       "0   ['Fig. 1', 'Figure 1', 'Figure 1:']   \n",
       "1   ['Fig. 2', 'Figure 2', 'Figure 2:']   \n",
       "2               ['Table 2', 'Table 2:']   \n",
       "3               ['Table 4', 'Table 4:']   \n",
       "4               ['Table 5', 'Table 5:']   \n",
       "5               ['Table 6', 'Table 6:']   \n",
       "6               ['Table 7', 'Table 7:']   \n",
       "7   ['Fig. 3', 'Figure 3', 'Figure 3:']   \n",
       "8   ['Fig. 4', 'Figure 4', 'Figure 4:']   \n",
       "10  ['Fig. 5', 'Figure 5', 'Figure 5:']   \n",
       "\n",
       "                                             contexts  \n",
       "0   [[], [' TokM  Masked Sentence A Masked Sentenc...  \n",
       "1   [[], [' A visualiza- tion of this construction...  \n",
       "2   [[' Table 2 shows top leaderboard entries as w...  \n",
       "3   [['0 Table 4: SWAG Dev and Test accuracies', '...  \n",
       "4   [['9 Table 5: Ablation over the pre-training t...  \n",
       "5   [[' Results on selected GLUE tasks are shown i...  \n",
       "6   [['5 - Table 7: CoNLL-2003 Named Entity Recogn...  \n",
       "7   [[], ['Figure 3: Differences in pre-training m...  \n",
       "8   [[], ['5 Illustrations of Fine-tuning on Diffe...  \n",
       "10  [[], ['1 Effect of Number of Training Steps Fi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the row if there is NaN\n",
    "image_discrption_df = image_discrption_df.dropna()\n",
    "image_discrption_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows with repeated discription\n",
    "image_discrption_df = image_discrption_df.drop_duplicates(subset='discription', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_discrption_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>discription</th>\n",
       "      <th>key_words</th>\n",
       "      <th>keys</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other_images/Figure_2_0.png</td>\n",
       "      <td>Figure 1: Overall pre-training and fine-tuning...</td>\n",
       "      <td>Figure 1:</td>\n",
       "      <td>['Fig. 1', 'Figure 1', 'Figure 1:']</td>\n",
       "      <td>[[], [' TokM  Masked Sentence A Masked Sentenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other_images/Figure_4_0.png</td>\n",
       "      <td>Figure 2: BERT input representation. The input...</td>\n",
       "      <td>Figure 2:</td>\n",
       "      <td>['Fig. 2', 'Figure 2', 'Figure 2:']</td>\n",
       "      <td>[[], [' A visualiza- tion of this construction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other_images/Figure_6_0.png</td>\n",
       "      <td>Table 2: SQuAD 1.1 results. The BERT ensemble\\...</td>\n",
       "      <td>Table 2:</td>\n",
       "      <td>['Table 2', 'Table 2:']</td>\n",
       "      <td>[[' Table 2 shows top leaderboard entries as w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other_images/Figure_6_8.png</td>\n",
       "      <td>Table 4: SWAG Dev and Test accuracies. ‘Human ...</td>\n",
       "      <td>Table 4:</td>\n",
       "      <td>['Table 4', 'Table 4:']</td>\n",
       "      <td>[['0 Table 4: SWAG Dev and Test accuracies', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other_images/Figure_7_0.png</td>\n",
       "      <td>Table 5: Ablation over the pre-training tasks ...</td>\n",
       "      <td>Table 5:</td>\n",
       "      <td>['Table 5', 'Table 5:']</td>\n",
       "      <td>[['9 Table 5: Ablation over the pre-training t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other_images/Table_8_3.png</td>\n",
       "      <td>Table 6: Ablation over BERT model size. #L = t...</td>\n",
       "      <td>Table 6:</td>\n",
       "      <td>['Table 6', 'Table 6:']</td>\n",
       "      <td>[[' Results on selected GLUE tasks are shown i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other_images/Figure_8_5.png</td>\n",
       "      <td>Table 7: CoNLL-2003 Named Entity Recognition r...</td>\n",
       "      <td>Table 7:</td>\n",
       "      <td>['Table 7', 'Table 7:']</td>\n",
       "      <td>[['5 - Table 7: CoNLL-2003 Named Entity Recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other_images/Figure_12_0.png</td>\n",
       "      <td>Figure 3: Differences in pre-training model ar...</td>\n",
       "      <td>Figure 3:</td>\n",
       "      <td>['Fig. 3', 'Figure 3', 'Figure 3:']</td>\n",
       "      <td>[[], ['Figure 3: Differences in pre-training m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other_images/Figure_14_0.png</td>\n",
       "      <td>Figure 4: Illustrations of Fine-tuning BERT on...</td>\n",
       "      <td>Figure 4:</td>\n",
       "      <td>['Fig. 4', 'Figure 4', 'Figure 4:']</td>\n",
       "      <td>[[], ['5 Illustrations of Fine-tuning on Diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>other_images/Figure_15_4.png</td>\n",
       "      <td>Figure 5: Ablation over number of training ste...</td>\n",
       "      <td>Figure 5:</td>\n",
       "      <td>['Fig. 5', 'Figure 5', 'Figure 5:']</td>\n",
       "      <td>[[], ['1 Effect of Number of Training Steps Fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0    other_images/Figure_2_0.png   \n",
       "1    other_images/Figure_4_0.png   \n",
       "2    other_images/Figure_6_0.png   \n",
       "3    other_images/Figure_6_8.png   \n",
       "4    other_images/Figure_7_0.png   \n",
       "5     other_images/Table_8_3.png   \n",
       "6    other_images/Figure_8_5.png   \n",
       "7   other_images/Figure_12_0.png   \n",
       "8   other_images/Figure_14_0.png   \n",
       "10  other_images/Figure_15_4.png   \n",
       "\n",
       "                                          discription  key_words  \\\n",
       "0   Figure 1: Overall pre-training and fine-tuning...  Figure 1:   \n",
       "1   Figure 2: BERT input representation. The input...  Figure 2:   \n",
       "2   Table 2: SQuAD 1.1 results. The BERT ensemble\\...  Table 2:    \n",
       "3   Table 4: SWAG Dev and Test accuracies. ‘Human ...  Table 4:    \n",
       "4   Table 5: Ablation over the pre-training tasks ...  Table 5:    \n",
       "5   Table 6: Ablation over BERT model size. #L = t...  Table 6:    \n",
       "6   Table 7: CoNLL-2003 Named Entity Recognition r...  Table 7:    \n",
       "7   Figure 3: Differences in pre-training model ar...  Figure 3:   \n",
       "8   Figure 4: Illustrations of Fine-tuning BERT on...  Figure 4:   \n",
       "10  Figure 5: Ablation over number of training ste...  Figure 5:   \n",
       "\n",
       "                                   keys  \\\n",
       "0   ['Fig. 1', 'Figure 1', 'Figure 1:']   \n",
       "1   ['Fig. 2', 'Figure 2', 'Figure 2:']   \n",
       "2               ['Table 2', 'Table 2:']   \n",
       "3               ['Table 4', 'Table 4:']   \n",
       "4               ['Table 5', 'Table 5:']   \n",
       "5               ['Table 6', 'Table 6:']   \n",
       "6               ['Table 7', 'Table 7:']   \n",
       "7   ['Fig. 3', 'Figure 3', 'Figure 3:']   \n",
       "8   ['Fig. 4', 'Figure 4', 'Figure 4:']   \n",
       "10  ['Fig. 5', 'Figure 5', 'Figure 5:']   \n",
       "\n",
       "                                             contexts  \n",
       "0   [[], [' TokM  Masked Sentence A Masked Sentenc...  \n",
       "1   [[], [' A visualiza- tion of this construction...  \n",
       "2   [[' Table 2 shows top leaderboard entries as w...  \n",
       "3   [['0 Table 4: SWAG Dev and Test accuracies', '...  \n",
       "4   [['9 Table 5: Ablation over the pre-training t...  \n",
       "5   [[' Results on selected GLUE tasks are shown i...  \n",
       "6   [['5 - Table 7: CoNLL-2003 Named Entity Recogn...  \n",
       "7   [[], ['Figure 3: Differences in pre-training m...  \n",
       "8   [[], ['5 Illustrations of Fine-tuning on Diffe...  \n",
       "10  [[], ['1 Effect of Number of Training Steps Fi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_discrption_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### figs in Method from previous code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>discription</th>\n",
       "      <th>key_words</th>\n",
       "      <th>keys</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other_images/Figure_2_0.png</td>\n",
       "      <td>Figure 1: Overall pre-training and fine-tuning...</td>\n",
       "      <td>Figure 1:</td>\n",
       "      <td>['Fig. 1', 'Figure 1', 'Figure 1:']</td>\n",
       "      <td>[[], [' TokM  Masked Sentence A Masked Sentenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other_images/Figure_4_0.png</td>\n",
       "      <td>Figure 2: BERT input representation. The input...</td>\n",
       "      <td>Figure 2:</td>\n",
       "      <td>['Fig. 2', 'Figure 2', 'Figure 2:']</td>\n",
       "      <td>[[], [' A visualiza- tion of this construction...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         image  \\\n",
       "0  other_images/Figure_2_0.png   \n",
       "1  other_images/Figure_4_0.png   \n",
       "\n",
       "                                         discription  key_words  \\\n",
       "0  Figure 1: Overall pre-training and fine-tuning...  Figure 1:   \n",
       "1  Figure 2: BERT input representation. The input...  Figure 2:   \n",
       "\n",
       "                                  keys  \\\n",
       "0  ['Fig. 1', 'Figure 1', 'Figure 1:']   \n",
       "1  ['Fig. 2', 'Figure 2', 'Figure 2:']   \n",
       "\n",
       "                                            contexts  \n",
       "0  [[], [' TokM  Masked Sentence A Masked Sentenc...  \n",
       "1  [[], [' A visualiza- tion of this construction...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_keys = ['Figure 1', 'Figure 2', 'Figure 2:']\n",
    "#method_keys = []\n",
    "# filter the rows if the 'keys' column contained method_keys and also print that key\n",
    "# Filter the DataFrame\n",
    "filtered_df = image_discrption_df[image_discrption_df['keys'].apply(lambda x: any(key in x for key in method_keys))]\n",
    "\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "   \n",
    "filtered_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a language model that has two key stages: pre-training and fine-tuning. In the pre-training phase, the model is trained on a range of tasks without labeled data. The fine-tuning phase involves using labeled data from specific tasks, which is where the pre-trained parameters of the BERT model are fine-tuned for targeted performance. \n",
      "\n",
      "A key feature of BERT is its model architecture, which is a multi-layer bidirectional Transformer encoder. The number of layers is represented as L, the hidden size as H, and the number of self-attention heads as A. Two model sizes are used: BERT BASE and BERT LARGE. The main distinction between these is that BERT uses bidirectional self-attention while other models use self-attention that only conditions to the context on the left of each token.\n",
      "\n",
      "BERT's input representation allows for representation of both single sentences or pairs of sentences. The first token in every sequence is a special classification token ([CLS]) that helps understand aggregated sequence representation. Sentence pairs have a special token ([SEP]) to separate them and to identify their separate embeddings (Segment A or B), as shown in Figure 2.\n",
      "\n",
      "During pre-training, BERT uses two unsupervised tasks. The first involves masking a certain portion of the input tokens and then predicting those masked tokens — this approach is called \"Masked LM\". BERT also trains for a \"Next Sentence Prediction (NSP)\" task, which helps the model understand the relationship between sentences. The final hidden vector of the special [CLS] token (C) is used for this prediction.\n",
      "\n",
      "To enhance performance on various tasks like question answering and natural language inference, the authors 'fine-tuned' BERT. The advantage of BERT's method is that a wide range of tasks can be handled by simply changing the inputs and outputs even in task-specific areas. Fine-tuning is much quicker and cheaper than pre-training; it can be completed in just an hour on a cloud TPU or a few hours on a GPU. \n",
      "\n",
      "Overall, BERT's approach combines unlabeled pre-training with task-specific fine-tuning, leading to a powerful and versatile model for NLP tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# use re to get the text after '### Detailed Method' otherwise(all pics matches the overall part)\n",
    "method_summary = re.split('### Detailed Method:', V1_summary)[1]\n",
    "print(method_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nBERT (Bidirectional Encoder Representations from Transformers) is a language model that has two key stages: pre-training and fine-tuning',\n",
       " ' In the pre-training phase, the model is trained on a range of tasks without labeled data',\n",
       " ' The fine-tuning phase involves using labeled data from specific tasks, which is where the pre-trained parameters of the BERT model are fine-tuned for targeted performance',\n",
       " ' \\n\\nA key feature of BERT is its model architecture, which is a multi-layer bidirectional Transformer encoder',\n",
       " ' The number of layers is represented as L, the hidden size as H, and the number of self-attention heads as A',\n",
       " ' Two model sizes are used: BERT BASE and BERT LARGE',\n",
       " ' The main distinction between these is that BERT uses bidirectional self-attention while other models use self-attention that only conditions to the context on the left of each token',\n",
       " \"\\n\\nBERT's input representation allows for representation of both single sentences or pairs of sentences\",\n",
       " ' The first token in every sequence is a special classification token ([CLS]) that helps understand aggregated sequence representation',\n",
       " ' Sentence pairs have a special token ([SEP]) to separate them and to identify their separate embeddings (Segment A or B), as shown in Figure 2',\n",
       " '\\n\\nDuring pre-training, BERT uses two unsupervised tasks',\n",
       " ' The first involves masking a certain portion of the input tokens and then predicting those masked tokens — this approach is called \"Masked LM\"',\n",
       " ' BERT also trains for a \"Next Sentence Prediction (NSP)\" task, which helps the model understand the relationship between sentences',\n",
       " ' The final hidden vector of the special [CLS] token (C) is used for this prediction',\n",
       " \"\\n\\nTo enhance performance on various tasks like question answering and natural language inference, the authors 'fine-tuned' BERT\",\n",
       " \" The advantage of BERT's method is that a wide range of tasks can be handled by simply changing the inputs and outputs even in task-specific areas\",\n",
       " ' Fine-tuning is much quicker and cheaper than pre-training; it can be completed in just an hour on a cloud TPU or a few hours on a GPU',\n",
       " \" \\n\\nOverall, BERT's approach combines unlabeled pre-training with task-specific fine-tuning, leading to a powerful and versatile model for NLP tasks\",\n",
       " '\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn method_summary into a list of sentences\n",
    "method_summary_list = method_summary.split('.')\n",
    "method_summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_i(item, filtered_df):\n",
    "    # if the item is in the column 'keys' of filtered_df, return the index of that row\n",
    "    for i in range(len(filtered_df)):\n",
    "        if item in filtered_df.iloc[i]['keys']:\n",
    "            return i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pic\n"
     ]
    }
   ],
   "source": [
    "# for each item in method_keys, use RE find it in method_summary\n",
    "# if found, print the item and the sentence that contains it\n",
    "used_indices = set()\n",
    "for item in method_keys:\n",
    "    for index, sentence in enumerate(method_summary_list):\n",
    "        if re.search(item, sentence):\n",
    "          \n",
    "            i = get_the_i(item, filtered_df)\n",
    "            # Check if this index has already been used\n",
    "            if i not in used_indices:\n",
    "                used_indices.add(i)\n",
    "\n",
    "                target_key = filtered_df['image'][i]\n",
    "                target_caption = filtered_df['discription'][i]\n",
    "                pic_adding = f'<br><img src=\"{target_key}\" alt=\"alt text\" style=\"max-width: 80%; height: auto;\"><br><sub style=\"color: gray;\">{target_caption}</sub><br>'\n",
    "\n",
    "\n",
    "                sentence = sentence + \". \" + pic_adding\n",
    "                print('adding pic')\n",
    "                method_summary_list[index] = sentence\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nBERT (Bidirectional Encoder Representations from Transformers) is a language model that has two key stages: pre-training and fine-tuning. In the pre-training phase, the model is trained on a range of tasks without labeled data. The fine-tuning phase involves using labeled data from specific tasks, which is where the pre-trained parameters of the BERT model are fine-tuned for targeted performance. \\n\\nA key feature of BERT is its model architecture, which is a multi-layer bidirectional Transformer encoder. The number of layers is represented as L, the hidden size as H, and the number of self-attention heads as A. Two model sizes are used: BERT BASE and BERT LARGE. The main distinction between these is that BERT uses bidirectional self-attention while other models use self-attention that only conditions to the context on the left of each token.\\n\\nBERT\\'s input representation allows for representation of both single sentences or pairs of sentences. The first token in every sequence is a special classification token ([CLS]) that helps understand aggregated sequence representation. Sentence pairs have a special token ([SEP]) to separate them and to identify their separate embeddings (Segment A or B), as shown in Figure 2. <br><img src=\"other_images/Figure_4_0.png\" alt=\"alt text\" style=\"max-width: 80%; height: auto;\"><br><sub style=\"color: gray;\">Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\n</sub><br>.\\n\\nDuring pre-training, BERT uses two unsupervised tasks. The first involves masking a certain portion of the input tokens and then predicting those masked tokens — this approach is called \"Masked LM\". BERT also trains for a \"Next Sentence Prediction (NSP)\" task, which helps the model understand the relationship between sentences. The final hidden vector of the special [CLS] token (C) is used for this prediction.\\n\\nTo enhance performance on various tasks like question answering and natural language inference, the authors \\'fine-tuned\\' BERT. The advantage of BERT\\'s method is that a wide range of tasks can be handled by simply changing the inputs and outputs even in task-specific areas. Fine-tuning is much quicker and cheaper than pre-training; it can be completed in just an hour on a cloud TPU or a few hours on a GPU. \\n\\nOverall, BERT\\'s approach combines unlabeled pre-training with task-specific fine-tuning, leading to a powerful and versatile model for NLP tasks.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate the sentences in method_summary_list into a string, add \".\" between each sentence\n",
    "new_method_summary = \".\".join(method_summary_list)\n",
    "new_method_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\n",
      "\n",
      "### Overview:\n",
      "\n",
      "The paper tackles the problematic area of question answering tasks from passages in Wikipedia, addressing the specific prediction of an answer text span within a given passage and tackling unanswerable questions. BERT, a novel, conceptually simple yet potent model, is presented as the paper's primary contribution. BERT is optimized for deep bidirectional architectures, offering applicability across a wide range of Natural Language Processing tasks, exemplified by achieving state-of-the-art results across eleven such tasks. The paper utilizes various datasets, such as the GLUE benchmark, SQuAD v1.1, and the SWAG dataset. Ablation studies were conducted to explore aspects of BERT and the importance of its deep bidirectionality was a notable discovery. Despite various model alterations, BERT's effectiveness remained evident, particularly for Named Entity Recognition tasks. The paper concludes by affirming BERT's simplicity and efficacy in achieving exceptional results on multiple NLP tasks, emphasizing the crucial role of unsupervised pre-training in today's language understanding systems.\n",
      "\n",
      "### Detailed Method:\n",
      "\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a language model that has two key stages: pre-training and fine-tuning. In the pre-training phase, the model is trained on a range of tasks without labeled data. The fine-tuning phase involves using labeled data from specific tasks, which is where the pre-trained parameters of the BERT model are fine-tuned for targeted performance. \n",
      "\n",
      "A key feature of BERT is its model architecture, which is a multi-layer bidirectional Transformer encoder. The number of layers is represented as L, the hidden size as H, and the number of self-attention heads as A. Two model sizes are used: BERT BASE and BERT LARGE. The main distinction between these is that BERT uses bidirectional self-attention while other models use self-attention that only conditions to the context on the left of each token.\n",
      "\n",
      "BERT's input representation allows for representation of both single sentences or pairs of sentences. The first token in every sequence is a special classification token ([CLS]) that helps understand aggregated sequence representation. Sentence pairs have a special token ([SEP]) to separate them and to identify their separate embeddings (Segment A or B), as shown in Figure 2. <br><img src=\"other_images/Figure_4_0.png\" alt=\"alt text\" style=\"max-width: 80%; height: auto;\"><br><sub style=\"color: gray;\">Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\n",
      "tion embeddings and the position embeddings.\n",
      "</sub><br>.\n",
      "\n",
      "During pre-training, BERT uses two unsupervised tasks. The first involves masking a certain portion of the input tokens and then predicting those masked tokens — this approach is called \"Masked LM\". BERT also trains for a \"Next Sentence Prediction (NSP)\" task, which helps the model understand the relationship between sentences. The final hidden vector of the special [CLS] token (C) is used for this prediction.\n",
      "\n",
      "To enhance performance on various tasks like question answering and natural language inference, the authors 'fine-tuned' BERT. The advantage of BERT's method is that a wide range of tasks can be handled by simply changing the inputs and outputs even in task-specific areas. Fine-tuning is much quicker and cheaper than pre-training; it can be completed in just an hour on a cloud TPU or a few hours on a GPU. \n",
      "\n",
      "Overall, BERT's approach combines unlabeled pre-training with task-specific fine-tuning, leading to a powerful and versatile model for NLP tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V2_summary = re.split('### Detailed Method:', V1_summary)[0] + '### Detailed Method:' + new_method_summary\n",
    "print(V2_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new_method_summary as a markdown file\n",
    "with open('V2_summary.md', 'w') as f:\n",
    "    f.write(V2_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop the key \"keys\" column in filtered_df and find the index of the sentence that contains the at least the key, add column 'sentence_index' to filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just test >> 不是很准确哦\n",
    "# import faiss\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# def build_faiss_index(embeddings):\n",
    "#     # Build FAISS index\n",
    "#     index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "#     index.add(embeddings)\n",
    "#     return index\n",
    "\n",
    "# def find_most_similar_sentence(query, index, model):\n",
    "#     # Encode the query sentence\n",
    "#     query_embedding = model.encode([query])[0]\n",
    "\n",
    "#     # Search for the most similar sentences\n",
    "#     _, idx = index.search(query_embedding.reshape(1, -1), k=1)\n",
    "\n",
    "#     # Get the index of the most similar sentence\n",
    "#     most_similar_idx = idx[0][0]\n",
    "\n",
    "#     # Return the most similar sentence\n",
    "#     return sentences[most_similar_idx]\n",
    "\n",
    "# # Example sentences\n",
    "# sentences = method_summary_list\n",
    "\n",
    "# # Load the sentence transformer model\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# # Encode all sentences\n",
    "# embeddings = model.encode(sentences)\n",
    "\n",
    "# # Build FAISS index\n",
    "# index = build_faiss_index(embeddings)\n",
    "\n",
    "# # Query sentence\n",
    "# query_sentence = \"['Fig. 2', 'Figure 2', 'Figure 2:']\"\n",
    "\n",
    "# # Find the most similar sentence\n",
    "# most_similar_sentence = find_most_similar_sentence(query_sentence, index, model)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Query Sentence:\", query_sentence)\n",
    "# print(\"Most Similar Sentence:\", most_similar_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training chatGPT >> Not efficient O(N) for using chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# import openai\n",
    "\n",
    "# openai.api_key = api_key  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_pic(caption,context):\n",
    "#     prompt = \"\"\"\n",
    "#     given a caption and the original context texts of a graph, and given a paragraph of methodology summary, please first judge if adding the graph could illustrate the summary better?  if yes, clearly point out which sentence the graph should added after in the summary?\n",
    "#     \"\"\"\n",
    "#     sample_format = \"\"\"ANSWER STRICTLY IN THE FOLLOWING FORMAT:\n",
    "#     YES OR NO\n",
    "#     ADD AFTER:\" \"\n",
    "#     \"\"\"\n",
    "\n",
    "#     chat_history = [\n",
    "#         {'role': 'system', 'content': 'you are a helpful assistant.'},\n",
    "#         {'role': 'user', 'content': prompt + 'caption: \"\"\"' + str(caption) + '\"\"\"' + 'contexts: \"\"\"' + str(context)  + '\"\"\"' + 'summary: \"\"\"' + method_summary + '\"\"\"' + sample_format},\n",
    "#         {'role': 'assistant', 'content': \"The class is:\"},\n",
    "#     ]\n",
    "\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         #model=\"gpt-4\",\n",
    "#         messages=chat_history\n",
    "#     )\n",
    "\n",
    "#     answer = response.choices[0].message.content\n",
    "\n",
    "#     return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each row, discription is the first parameter and contexts is the second parameter, apply add_pic function\n",
    "# image_discrption_df['answer'] = image_discrption_df.apply(lambda x: add_pic(x['discription'],x['contexts']), axis=1)\n",
    "# image_discrption_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # just show the last column\n",
    "# for i in range(5):\n",
    "#     print(image_discrption_df['answer'][i][16:-1])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read V1.5_summary as text\n",
    "# with open ('V1_summary.md', 'r') as f:\n",
    "#     V1_summary = f.read()\n",
    "\n",
    "# print(V1_summary)\n",
    "\n",
    "# # 开始正式加图\n",
    "# #len_pic_adding = 0\n",
    "# for i in range(len(filtered_df)):\n",
    "#     target_key = filtered_df['image'][i]\n",
    "#     target_caption = filtered_df['discription'][i]\n",
    "#     pic_adding = f'<br><img src=\"{target_key}\" alt=\"alt text\" style=\"max-width: 80%; height: auto;\"><br><sub style=\"color: gray;\">{target_caption}</sub><br>'\n",
    "    \n",
    "#     ## need similarity search 来找到对应的句子\n",
    "#     doc = image_discrption_df['answer'][i][16:-1]\n",
    "#     if_found = V1_summary.find(doc)\n",
    "#     if if_found > 0:\n",
    "#         end_index = V1_summary.find(doc) + len(doc)\n",
    "#         # # make sure pic adding is not in the middle of a word\n",
    "#         # while V1_summary[end_index] != '\\n':\n",
    "#         #     end_index += 1\n",
    "\n",
    "#         V1_summary = V1_summary[:end_index]  + pic_adding  + V1_summary[end_index:]\n",
    "#         #len_pic_adding += len(pic_adding)\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "# V1_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save V1_summary as markdown file\n",
    "# with open('V2_summary.md', 'w') as f:\n",
    "#     f.write(V1_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish runing get_V2.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(\"finish runing get_V2.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
