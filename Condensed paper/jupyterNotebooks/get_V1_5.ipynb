{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* V2 means add images(tables/formulas/graphs) to V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\n",
      "\n",
      "### Overview:\n",
      "\n",
      "The paper tackles the problematic area of question answering tasks from passages in Wikipedia, addressing the specific prediction of an answer text span within a given passage and tackling unanswerable questions. BERT, a novel, conceptually simple yet potent model, is presented as the paper's primary contribution. BERT is optimized for deep bidirectional architectures, offering applicability across a wide range of Natural Language Processing tasks, exemplified by achieving state-of-the-art results across eleven such tasks. The paper utilizes various datasets, such as the GLUE benchmark, SQuAD v1.1, and the SWAG dataset. Ablation studies were conducted to explore aspects of BERT and the importance of its deep bidirectionality was a notable discovery. Despite various model alterations, BERT's effectiveness remained evident, particularly for Named Entity Recognition tasks. The paper concludes by affirming BERT's simplicity and efficacy in achieving exceptional results on multiple NLP tasks, emphasizing the crucial role of unsupervised pre-training in today's language understanding systems.\n",
      "\n",
      "### Detailed Method:\n",
      "\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a language model that has two key stages: pre-training and fine-tuning. In the pre-training phase, the model is trained on a range of tasks without labeled data. The fine-tuning phase involves using labeled data from specific tasks, which is where the pre-trained parameters of the BERT model are fine-tuned for targeted performance. \n",
      "\n",
      "A key feature of BERT is its model architecture, which is a multi-layer bidirectional Transformer encoder. The number of layers is represented as L, the hidden size as H, and the number of self-attention heads as A. Two model sizes are used: BERT BASE and BERT LARGE. The main distinction between these is that BERT uses bidirectional self-attention while other models use self-attention that only conditions to the context on the left of each token.\n",
      "\n",
      "BERT's input representation allows for representation of both single sentences or pairs of sentences. The first token in every sequence is a special classification token ([CLS]) that helps understand aggregated sequence representation. Sentence pairs have a special token ([SEP]) to separate them and to identify their separate embeddings (Segment A or B), as shown in Figure 2.\n",
      "\n",
      "During pre-training, BERT uses two unsupervised tasks. The first involves masking a certain portion of the input tokens and then predicting those masked tokens â€” this approach is called \"Masked LM\". BERT also trains for a \"Next Sentence Prediction (NSP)\" task, which helps the model understand the relationship between sentences. The final hidden vector of the special [CLS] token (C) is used for this prediction.\n",
      "\n",
      "To enhance performance on various tasks like question answering and natural language inference, the authors 'fine-tuned' BERT. The advantage of BERT's method is that a wide range of tasks can be handled by simply changing the inputs and outputs even in task-specific areas. Fine-tuning is much quicker and cheaper than pre-training; it can be completed in just an hour on a cloud TPU or a few hours on a GPU. \n",
      "\n",
      "Overall, BERT's approach combines unlabeled pre-training with task-specific fine-tuning, leading to a powerful and versatile model for NLP tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read V1_summary as text\n",
    "with open ('V1_summary.md', 'r') as f:\n",
    "    V1_summary = f.read()\n",
    "\n",
    "print(V1_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1_summary = V1_summary + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(V1_summary.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add formula pic first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read math_ocr \n",
    "with open ('math_ocr_text.txt', 'r') as f:\n",
    "    math_ocr_text = f.read()\n",
    "\n",
    "# convert imgae_discription_text to dictionary\n",
    "math_ocr_dict = eval(math_ocr_text)\n",
    "math_ocr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using similarity_score_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No math detected\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('method_summary.md')\n",
    "\n",
    "#from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "if math_ocr_dict != {}:\n",
    "    \n",
    "    documents = loader.load()\n",
    "    #text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size = 80,\n",
    "        chunk_overlap  = 20,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False,\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .65})\n",
    "\n",
    "\n",
    "    # read the query from image_discription_dict value\n",
    "    query_list = list(math_ocr_dict.values()) \n",
    "\n",
    "    doc_list = []\n",
    "    for query in query_list:\n",
    "        doc = retriever.get_relevant_documents(query)\n",
    "        if len(doc) == 0:\n",
    "            doc_list.append('NA')\n",
    "        else:\n",
    "            doc_list.append(doc[0].page_content)\n",
    "\n",
    "    # get the non-NA doc_list\n",
    "    target_doc = [doc for doc in doc_list if doc != 'NA']\n",
    "\n",
    "    target_index = [i for i, doc in enumerate(doc_list) if doc != 'NA']\n",
    "    # Get the keys based on the index list\n",
    "    keys_from_index = [list(math_ocr_dict.keys())[index] for index in target_index]\n",
    "\n",
    "    #len_pic_adding = 0\n",
    "    for i in range(len(target_doc)):\n",
    "        target_key = keys_from_index[i]\n",
    "        pic_adding = f'<br><img src=\"{target_key}\" alt=\"alt text\" style=\"max-width: 60%; height: auto;\"><br>'\n",
    "        \n",
    "        doc = target_doc[i]\n",
    "        end_index = V1_summary.find(doc) + len(doc)\n",
    "        # make sure pic adding is not in the middle of a word\n",
    "        while V1_summary[end_index] != '\\n':\n",
    "            end_index += 1\n",
    "\n",
    "        V1_summary = V1_summary[:end_index]  + pic_adding  + V1_summary[end_index:]\n",
    "\n",
    "\n",
    "else:\n",
    "    print('No math detected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the test_V1_summary to V1.5_summary.md(add math images)\n",
    "with open('V1.5_summary.md', 'w') as f:\n",
    "    f.write(V1_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the query from image_discription_dict value\n",
    "# query_list = list(math_ocr_dict.values()) \n",
    "\n",
    "# doc_list = []\n",
    "# for query in query_list:\n",
    "#     doc = retriever.get_relevant_documents(query)\n",
    "#     if len(doc) == 0:\n",
    "#         doc_list.append('NA')\n",
    "#     else:\n",
    "#         doc_list.append(doc[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the non-NA doc_list\n",
    "# target_doc = [doc for doc in doc_list if doc != 'NA']\n",
    "# target_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_index = [i for i, doc in enumerate(doc_list) if doc != 'NA']\n",
    "# # Get the keys based on the index list\n",
    "# keys_from_index = [list(math_ocr_dict.keys())[index] for index in target_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the end index of target_doc in V1_summary\n",
    "# end_index_list = []\n",
    "# for doc in target_doc:\n",
    "#     end_index = V1_summary.find(doc) + len(doc)\n",
    "#     end_index_list.append(end_index)\n",
    "\n",
    "# end_index_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## cheating part>> will fix later on\n",
    "# image_discription_dict['math_images/1_0.png'] = \" \"\n",
    "# image_discription_dict['math_images/2_0.png'] = \" \"\n",
    "\n",
    "# image_discription_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #len_pic_adding = 0\n",
    "# for i in range(len(target_doc)):\n",
    "#     target_key = keys_from_index[i]\n",
    "#     pic_adding = f'<br><img src=\"{target_key}\" alt=\"alt text\" style=\"max-width: 60%; height: auto;\"><br>'\n",
    "    \n",
    "#     doc = target_doc[i]\n",
    "#     end_index = V1_summary.find(doc) + len(doc)\n",
    "#     # make sure pic adding is not in the middle of a word\n",
    "#     while V1_summary[end_index] != '\\n':\n",
    "#         end_index += 1\n",
    "\n",
    "#     V1_summary = V1_summary[:end_index]  + pic_adding  + V1_summary[end_index:]\n",
    "#     #len_pic_adding += len(pic_adding)\n",
    "\n",
    "# V1_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_for test_\n",
    "\n",
    "for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #V1_summary[index[0]:index[1]]\n",
    "# target_key = 'other_images/Figure_2_0.png'\n",
    "# pic_adding = '![Image](' + target_key +')'\n",
    "# pic_diescription = '_' + image_discription_dict[target_key] + '_'\n",
    "\n",
    "# # find the end_index and add adding to V1_summary\n",
    "# end_index = index[1]\n",
    "# test_V1_summary = V1_summary[:end_index] + pic_adding + V1_summary[end_index:]\n",
    "# test_V1_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the test_V1_summary to V1.5_summary.md(add math images)\n",
    "# with open('V1.5_summary.md', 'w') as f:\n",
    "#     f.write(V1_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](other_images/Figure_2_0.png)<sub>Image Credit</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thsi sis just some test tetxs<br><img src=\"other_images/Figure_2_0.png\" alt=\"alt text\" style=\"max-width: 80%; height: auto;\"><br><sub style=\"color: gray;\">1723hdfhdbfajdbfkajdfadImage Credit</sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish runing get_V1_5.ipynb\n"
     ]
    }
   ],
   "source": [
    "print(\"finish runing get_V1_5.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add other images\n",
    "### 1. ç”¨image captionåŠ threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read image_discription \n",
    "# with open ('image_discription_text.txt', 'r') as f:\n",
    "#     image_discription_text = f.read()\n",
    "\n",
    "# # convert imgae_discription_text to dictionary\n",
    "# image_discription_dict = eval(image_discription_text)\n",
    "# image_discription_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read image_discription \n",
    "# with open ('image_discription_text.txt', 'r') as f:\n",
    "#     image_discription_text = f.read()\n",
    "\n",
    "# # convert imgae_discription_text to dictionary\n",
    "# image_discription_dict = eval(image_discription_text)\n",
    "# image_discription_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever2 = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the query from image_discription_dict value\n",
    "# query_list = list(image_discription_dict.values()) \n",
    "\n",
    "# doc_list = []\n",
    "# for query in query_list:\n",
    "#     doc = retriever2.get_relevant_documents(query)\n",
    "#     if len(doc) == 0:\n",
    "#         doc_list.append('NA')\n",
    "#     else:\n",
    "#         doc_list.append(doc[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the non-NA doc_list\n",
    "# target_doc = [doc for doc in doc_list if doc != 'NA']\n",
    "\n",
    "# target_index = [i for i, doc in enumerate(doc_list) if doc != 'NA']\n",
    "\n",
    "# # Get the keys based on the index list\n",
    "# keys_from_index = [list(image_discription_dict.keys())[index] for index in target_index]\n",
    "# keys_from_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(target_doc)):\n",
    "#     target_key = keys_from_index[i]\n",
    "#     #pic_adding = f'<br><img src=\"{target_key}\" alt=\"alt text\" style=\"max-width: auto; height: auto;\"><br>'\n",
    "#     pic_adding = f'<br><img src=\"{target_key}\" alt=\"alt text\" style=\"max-width: 80%; height: auto;\"><br><sub style=\"color: gray;\">{image_discription_dict[target_key]}</sub><br>'\n",
    "    \n",
    "#     doc = target_doc[i]\n",
    "#     end_index = V1_summary.find(doc) + len(doc)\n",
    "#     # make sure pic adding is not in the middle of a word\n",
    "#     while V1_summary[end_index] != '\\n':\n",
    "#         end_index += 1\n",
    "\n",
    "#     V1_summary = V1_summary[:end_index+1]  + pic_adding  + V1_summary[end_index+1:]\n",
    "#     #len_pic_adding += len(pic_adding)\n",
    "\n",
    "# V1_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the test_V1_summary to V2_summary.md(add other images)\n",
    "# with open('V2_summary.md', 'w') as f:\n",
    "#     f.write(V1_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
